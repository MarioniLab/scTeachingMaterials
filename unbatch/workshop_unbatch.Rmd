---
title: Correcting batch effects in single-cell RNA-seq data
author: Aaron T. L. Lun
output: 
    BiocStyle::html_document:
        fit_caption: false
---

```{r style, echo=FALSE, results='hide', message=FALSE}
library(BiocStyle)
library(knitr)
opts_chunk$set(error=FALSE, message=FALSE, warning=FALSE)
#options(bitmapType="cairo", width=100) # if transparencies don't work on your machine.
```

# Introduction

Large scRNA-seq datasets are often generated in multiple batches due to logistical constraints.
This results in batch effects due to uncontrollable differences in the processing of different batches, e.g., changes in operator, differences in reagent quality.
Batch effects are problematic as they can be major drivers of heterogeneity in the data, masking the relevant biological differences and complicating interpretation of the results.

Here, we will explore a few methods for removing these batch effects.
We will be using a haematopoiesis dataset, generated by two groups with different technologies and involving different parts of the haematopoietic lineage.

# Setting up the Nestorowa data

<!--
```{r, eval=FALSE}
# Download and read the counts, metadata of Nestorowa et al. 2016
fname <- "GSE81682_HTSeq_counts.txt.gz"
download.file("https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE81682&format=file&file=GSE81682%5FHTSeq%5Fcounts%2Etxt%2Egz", fname)
```
-->

## Data input

Here, we are using data from Nestorowa et al. (https://doi.org/10.1182/blood-2016-05-716480).
We load in the counts:

```{r}
dataN <- read.table("GSE81682_HTSeq_counts.txt.gz", header=TRUE, row.names=1, check.names=FALSE)
dataN <- as.matrix(dataN)
```

We convert this to a sparse matrix to save space:

```{r}
library(Matrix)
dataN <- as(dataN, "dgCMatrix")
dim(dataN)
```

... and we get rid of non-genes at the end:


```{r}
dataN <- dataN[!grepl("^_", rownames(dataN)),]
dim(dataN)
```

We construct a `SingleCellExperiment` object:

```{r}
library(SingleCellExperiment)
sceN <- SingleCellExperiment(list(counts=dataN))
```

... and throw in some spike-in information:

```{r}
isSpike(sceN, "ERCC") <- grepl("^ERCC", rownames(sceN))
```

We also add information about the cell type:

```{r}
sceN$CellType <- sub("_.*", "", colnames(sceN))
```

<div class="alert alert-warning">
**Exercise:**

```{r}
# What does sub do?
sub("happy", "sad", "Aaron is happy")
```
</div>

## Normalizing for cell-specific biases

The authors have already done the quality control for us, so we skip straight to the normalization.
First, we break up the dataset with `quickCluster`:

```{r}
library(scran)
clustersN <- quickCluster(sceN, method="igraph")
table(clustersN)
```

Then we apply the deconvolution method:

```{r}
sceN <- computeSumFactors(sceN, clusters=clustersN)
summary(sizeFactors(sceN))
```

We plot the size factors against the library sizes as a sanity check:

```{r}
plot(Matrix::colSums(counts(sceN)), sizeFactors(sceN), log="xy")
```

We also generate size factors for the spike-ins separately.
Remember, it's fine that the spike-in size factors are not well-correlated to the deconvolution size factors.

```{r}
sceN <- computeSpikeFactors(sceN, general.use=FALSE, type="ERCC")
plot(sizeFactors(sceN), sizeFactors(sceN, "ERCC"), log="xy")
```

Finally, we compute log-normalized expression values:

```{r}
sceN <- normalize(sceN)
```

## Detecting highly variable genes

Let's pick out some highly variable genes.

```{r}
fitN <- trendVar(sceN, parametric=TRUE, loess.args=list(span=0.2))
decN <- decomposeVar(sceN, fitN)
head(decN)
```

Having a look at them on the plot:

```{r}
plot(decN$mean, decN$total, xlab="Mean log-expression",
    ylab="Variance of log-expression")
points(fitN$mean, fitN$var, col="red", pch=16)
curve(fitN$trend(x), col="red", add=TRUE)
```

```{r, echo=FALSE}
# Clearing the garbage to free up memory
gc()
```

# Setting up the Paul data

<!--
```{r, eval=FALSE}
# Download and read the counts from Paul et al. 2016
fname <- "umitab_Amit.txt.gz"
download.file("https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE72857&format=file&file=GSE72857%5Fumitab%2Etxt%2Egz", fname)
```
-->

## Data input

Here, we are using data from Paul et al. (https://doi.org/10.1016/j.cell.2015.11.013).
We load in the counts:

```{r}
dataP <- read.table("umitab_Amit.txt.gz", header=TRUE, row.names=1, check.names=FALSE)
dataP <- as.matrix(dataP)
```

We convert this to a sparse matrix, and take a random set of 2000 cells for the sake of speed:

```{r}
set.seed(0)
chosen <- sample(ncol(dataP), 2000)
dataP <- as(dataP[,chosen], "dgCMatrix")
dim(dataP)
```

We convert the gene names back to Ensembl:

```{r}
library(org.Mm.eg.db)
symbols <- sub(".*;", "", rownames(dataP))
ens <- mapIds(org.Mm.eg.db, keys=symbols, keytype="SYMBOL", column="ENSEMBL")
```

We remove duplicates or genes without any Ensembl:

```{r}
keep <- !is.na(ens) & !duplicated(ens)
summary(keep)
dataP <- dataP[keep,]
rownames(dataP) <- unname(ens[keep])
```

We now construct a `SingleCellExperiment` object:

```{r}
sceP <- SingleCellExperiment(list(counts=dataP))
```

<div class="alert alert-warning">
**Exercise:**

```{r}
# What does unname do?
ens[1:10]
#H# unname()
#A# unname(ens)[1:10]
```
</div>

## Removing low-quality cells

We will remove some low quality cells here:

```{r}
library(scater)
sceP <- calculateQCMetrics(sceP)
multiplot(cols=2,
    plotColData(sceP, "log10_total_counts"),
    plotColData(sceP, "log10_total_features_by_counts"))
```

We'll apply our outlier strategy:

```{r}
low.total <- isOutlier(sceP$log10_total_counts, type="lower", nmads=3)
low.nexpr <- isOutlier(sceP$log10_total_features_by_counts, type="lower", nmads=3)
discard <- low.total | low.nexpr
data.frame(LowLib=sum(low.total), LowNexprs=sum(low.nexpr), Lost=sum(discard))
```

... and discard the offending cells:

```{r}
sceP <- sceP[,!discard]
```

<div class="alert alert-warning">
**Exercise:**

```{r}
# What is the minimum retained total counts, or total number of features?
#H# sceP$total_counts
#H# sceP$total_features_by_counts
#A# min(sceP$total_counts)
#A# min(sceP$total_features_by_counts)
```
</div>

## Normalizing for cell-specific biases

Again, we break up the dataset with `quickCluster` - note `min.mean` is set lower than the default, as we are dealing with UMI counts.

```{r}
library(scran)
clustersP <- quickCluster(sceP, method="igraph", min.mean=0.1)
table(clustersP)
```

Then we apply the deconvolution method:

```{r}
sceP <- computeSumFactors(sceP, clusters=clustersP, min.mean=0.1)
summary(sizeFactors(sceP))
```

We plot the size factors against the library sizes as a sanity check:

```{r}
plot(Matrix::colSums(counts(sceP)), sizeFactors(sceP), log="xy")
```

Finally, we compute log-normalized expression values:

```{r}
sceP <- normalize(sceP)
```

## Detecting highly variable genes

Let's pick out some highly variable genes. 
There aren't any spike-ins, so we'll have to use `use.spikes=FALSE`.

```{r}
fitP <- trendVar(sceP, use.spikes=FALSE, loess.args=list(span=0.01))
decP <- decomposeVar(sceP, fitP)
head(decP)
```

Having a look at them on the plot:

```{r}
plot(decP$mean, decP$total, xlab="Mean log-expression",
    ylab="Variance of log-expression")
curve(fitP$trend(x), col="blue", add=TRUE)
```

<div class="alert alert-warning">
**Exercise:**

```{r}
# What happens if you play around with span?
fitP2 <- trendVar(sceP, use.spikes=FALSE, loess.args=list(span=0.2))
plot(decP$mean, decP$total, xlab="Mean log-expression",
    ylab="Variance of log-expression")
curve(fitP2$trend(x), col="blue", add=TRUE)
```
</div>

# Merging a set of highly variable genes

First, we need to obtain a common set of genes based on the Ensembl IDs.
This is why Ensembl IDs are so useful - they are, by definition, constant across experiments.

```{r}
common <- intersect(rownames(decN), rownames(decP))
length(common)
```

We combine the statistics using `combineVar`.
This uses Fisher's method to combine p-values across batches.
We take all genes that are significant at a FDR of 5%.

```{r}
combined <- combineVar(decP[common,], decN[common,])
to.use <- common[combined$FDR <= 0.05]
length(to.use)
```

# Removing the batch effect

## With a linear model

The simplest approach is to remove batch effects with a linear model, e.g., using `removeBatchEffect`.
In this case, we're basically shifting the expression values so that the mean is the same between batches for every gene.
Let's try doing that:

```{r}
library(limma)
batch <- rep(c("N", "P"), c(ncol(sceN), ncol(sceP)))
together <- cbind(logcounts(sceN)[to.use,], logcounts(sceP)[to.use,])
together <- removeBatchEffect(together, batch=batch)
```

We set up a `SingleCellExperiment` object again:

```{r}
sceT <- SingleCellExperiment(list(exprs=together))
sceT$Batch <- batch
```

...and we make a PCA plot to look at:

```{r}
sceT <- runPCA(sceT, exprs_values="exprs", method="irlba",
    ncomponents=50, feature_set=to.use)
plotPCA(sceT, colour_by="Batch")
```

Trying out a _t_-SNE plot:

```{r}
sceT <- runTSNE(sceT, use_dimred="PCA", perplexity=80)
plotTSNE(sceT, colour_by="Batch")
```

Even after `removeBatchEffect`, a strong batch effect still remains, for two reasons:

- differences in variance between UMI and read counts
- differences in cell type composition between data sets

## With MNN correction

A more sophisticated approach involves using the "mutual nearest neighbors" correction method (see https://www.nature.com/articles/nbt.4091/figures/1).
This involves finding pairs of cells - one per batch - where one cell in one batch is the nearest neighbour of a cell in the other batch, **and vice versa**.
Such pairs are likely to represent cells of the same type or state; we can use this to compute correction vectors between the two batches.

```{r}
corrected <- mnnCorrect(counts(sceN)[to.use,], counts(sceP)[to.use,], sigma=0.05)
```

We can combine the corrected matrices into a single object:

```{r}
out <- do.call(cbind, corrected$corrected)
colnames(out) <- NULL
sceM <- SingleCellExperiment(list(exprs=out),
    colData=DataFrame(Batch=batch))
```

... and we run a PCA using `r CRANpkg("irlba")` to get the first 50 PCs.

```{r}
sceM <- runPCA(sceM, exprs_values="exprs", method="irlba",
    ncomponents=50, feature_set=to.use)
```

Plotting it indicates that the batch effect has been removed from the first two PCs.
In particular, the Paul dataset has been integrated into the Nestorowa dataset:

```{r}
plotPCA(sceM, colour_by="Batch")
```

The _t_-SNE is a bit messier, but it shows the point:

```{r}
sceM <- runTSNE(sceM, use_dimred="PCA", perplexity=80)
plotTSNE(sceM, colour_by="Batch")
```

The corrected values can be used for clustering, but it not for DE as the values are not interpretable as log-counts.
We suggest using the learnt structure (clusters) with the batch of origin as a blocking factor in the design matrix.

# Session information

```{r}
sessionInfo()
```
